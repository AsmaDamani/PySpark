{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.3.0\n",
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8MB 76kB/s  eta 0:00:012   |███▌                            | 24.2MB 6.7MB/s eta 0:00:29     |██████▊                         | 46.1MB 6.4MB/s eta 0:00:28     |█████████▉                      | 66.9MB 5.3MB/s eta 0:00:29.3MB 5.3MB/s eta 0:00:27MB/s eta 0:00:20MB/s eta 0:00:20███▎                  | 90.1MB 5.2MB/s eta 0:00:25     |█████████████▋                  | 92.3MB 5.2MB/s eta 0:00:25     |███████████████                 | 102.9MB 34.3MB/s eta 0:00:04     |████████████████▉               | 114.8MB 6.6MB/s eta 0:00:16     |█████████████████               | 116.2MB 6.6MB/s eta 0:00:16     |█████████████████▊              | 120.4MB 6.6MB/s eta 0:00:15     |██████████████████▍             | 125.1MB 34.1MB/s eta 0:00:03     |██████████████████▌             | 125.8MB 34.1MB/s eta 0:00:03     |██████████████████▋             | 126.9MB 34.1MB/s eta 0:00:03     |██████████████████▊             | 127.5MB 34.1MB/s eta 0:00:03     |███████████████████             | 129.3MB 34.1MB/s eta 0:00:03     |███████████████████▋            | 133.7MB 34.8MB/s eta 0:00:03     |████████████████████            | 135.5MB 34.8MB/s eta 0:00:03     |████████████████████▋           | 140.5MB 34.8MB/s eta 0:00:03     |████████████████████▊           | 141.2MB 34.8MB/s eta 0:00:03     |████████████████████▉           | 142.2MB 5.2MB/s eta 0:00:15     |█████████████████████           | 143.2MB 5.2MB/s eta 0:00:15     |█████████████████████▏          | 144.3MB 5.2MB/s eta 0:00:15     |█████████████████████▎          | 144.8MB 5.2MB/s eta 0:00:14     |█████████████████████▌          | 146.6MB 5.2MB/s eta 0:00:14     |█████████████████████▊          | 147.9MB 5.2MB/s eta 0:00:14     |██████████████████████          | 149.2MB 5.2MB/s eta 0:00:14     |██████████████████████          | 149.8MB 5.2MB/s eta 0:00:13     |██████████████████████          | 150.5MB 5.2MB/s eta 0:00:13     |██████████████████████▍         | 152.7MB 33.5MB/s eta 0:00:02     |██████████████████████▌         | 153.4MB 33.5MB/s eta 0:00:02     |███████████████████████▌        | 159.9MB 33.5MB/s eta 0:00:02     |███████████████████████▋        | 160.6MB 33.5MB/s eta 0:00:02     |███████████████████████▊        | 161.2MB 33.5MB/s eta 0:00:02     |███████████████████████▉        | 161.9MB 33.5MB/s eta 0:00:02     |████████████████████████        | 163.3MB 33.4MB/s eta 0:00:02     |████████████████████████        | 164.0MB 33.4MB/s eta 0:00:02     |████████████████████████▌       | 166.8MB 33.4MB/s eta 0:00:02     |████████████████████████▋       | 167.2MB 33.4MB/s eta 0:00:02     |████████████████████████▊       | 168.4MB 33.4MB/s eta 0:00:02     |█████████████████████████       | 169.7MB 33.4MB/s eta 0:00:02     |█████████████████████████       | 170.2MB 33.4MB/s eta 0:00:02     |█████████████████████████▉      | 176.1MB 5.3MB/s eta 0:00:08   | 186.4MB 6.7MB/s eta 0:00:05   | 191.2MB 6.7MB/s eta 0:00:04��█████████▊   | 195.7MB 5.4MB/s eta 0:00:05     |█████████████████████████████▏  | 198.3MB 5.4MB/s eta 0:00:04     |█████████████████████████████▋  | 201.2MB 5.4MB/s eta 0:00:04��█████████████████████  | 203.8MB 5.4MB/s eta 0:00:031.9MB 5.3MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 30.3MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install pyspark\n",
    "import findspark\n",
    "import pyspark\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/resources/jupyterlab/labs/BD0211EN/LabData/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count() #to get no of rows,elements. An action(returns a value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesWithSpark = readme.filter(lambda line: \"Spark\" in line) #transformation, filter returns a new RDD containing only the elements that satisfy a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.count() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " 'You can find the latest Spark documentation, including a programming']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.filter(lambda line: \"Spark\" in line).count() #transformation and action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = readme.map(lambda line: len(line.split())) #map is a transformation operation, applies to each element of RDD andreturns the result as new RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 14, 13, 11]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_func.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_func.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line: len(line.split())).reduce(lambda x,y : x if(x > y) else y) #reeduce - aggregation of elements using a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP = readme.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#', 'Apache', 'Spark'],\n",
       " [],\n",
       " ['Spark',\n",
       "  'is',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'and',\n",
       "  'general',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'system',\n",
       "  'for',\n",
       "  'Big',\n",
       "  'Data.',\n",
       "  'It',\n",
       "  'provides']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlatMap_func = readme.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Apache', 'Spark']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_func.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlatMap_MAP = readme.flatMap(lambda line: line.split()).map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_MAP.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlatMap_MAP_reduceByKey = readme.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b) \n",
    "# reduceByKey combine values with the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 14)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_MAP_reduceByKey.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the collect function brings all of the data into the driver node. For a small dataset, this is acceptable but, \n",
    "for a large dataset this can cause an Out Of Memory error. It is recommended to use collect() for testing only. \n",
    "The safer approach is to use the take() function e.g. print take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('your', 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_MAP_reduceByKey.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 21)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_MAP_reduceByKey.max(lambda x:x[1]) # to get most frquent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 21)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlatMap_MAP_reduceByKey.reduce(lambda a, b: a if (a[1] > b[1]) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
